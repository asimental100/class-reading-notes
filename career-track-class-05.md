Web Scarpping with Javascrit
  1. HTTP clients - are tools capable of sending a request to a server and then receive a response from it. Almost every tool that will be discussed uses an HTTP client under the hood, to query the server of the website that you will attempt to scrape. 
    1.a. Request - is one of the most widely used HTTP clients in the Javascript ecosystem, however, though currently, the author of the Request library has officially declared that it is deprecated. This does not mean it is unusable, quite a lot of libraries still use it, and it is every bit worth using. It is fairly simple to make an HTTP request with Request. You can find the Request library at https://github.com/request/request and installing it is as simple as running npm install request.
    1.b. Axios - is a promise-based HTTP client that runs both in the browser and NodeJS. If you use Typescript, then axios has you covered with built-in types. Making an HTTP request with Axios is straight forward, it ships with promise support by default as opposed to utilizing callbacks in Request. You can find Axios library at https://github.com/axios/axios and installing Axios is as simple as npm install axios. 
    1.c. Superagent - Much like Axios, Superagent is another robust HTTP client that has support for promises and the async/await syntax sugar. It has a fairly straightforward API like Axios, but Superagent has more dependencies and is less popular. You can find the Superagent library at https://github.com/visionmedia/superagent and installing Superagent is as simple as npm install superagent.
  2. Regular Expressions - The simplest way to get started with web scraping without any dependencies is to use a bunch of regular expressions on the HTML string that you receive by querying a webpage using an HTTP client, but there is a big tradeoff. Regular Expressions aren't as flexible and quite a lot of people both professionals and amateurs struggle with writing the correct regular expression.
  3. Cherio - Cheerio is an efficient and light library which allows you to use the rich and powerful API of JQuery on the server-side. If you have used JQuery previously then you will feel right at home with Cheerio, it removes all the DOM inconsistencies and browser-related features and exposes an efficient API to parse and manipulate the DOM. install Cheerio and axios by running: "npm install cheerio axios"
    3.a. Cheerio is very similar to how you'd use JQuery. However, though it does not work the same way that a web browser works, which means it does not:
      3.a.1. Render any of the parsed or manipulated DOM elements.
      3.a.2. Apply CSS or load any external resource.
      3.a.3. Execute javascript.
  4. JSDOM - a pure Javascript implementation of the Document Object Model to be used in NodeJS, as mentioned previously the DOM is not available to Node, so JSDOM is the closest you can get. It more or less emulates the browser.
  5. Puppeteer - as the name implies, allows you to manipulate the browser programmatically just like how a puppet would be manipulated by its puppeteer. It achieves this by providing a developer with a high-level API to control a headless version of Chrome by default and can be configured to run non-headless.
    5.a. Puppeteer is particularly more useful than the aforementioned tools because it allows you to crawl the web as if a real person were interacting with a browser. This opens up a few possibilites that weren't there before:
      5.a.1. You can get screenshots or generate PDFs of pages..
      5.a.2. You could crawl a Single Page Application and generate pre-rendered content..
      5.a.3. Automate a lot of different user interactions like keyboard inputs, form submissions, navigation, etc.
      5.a.4. It could also play a big role in a lot of other tasks outside the scope of web crawling like UI testing, assist performance optimization, etc.
    5.b. It's quite often that you would want to take screenshots of websites, perhaps to get to know about a competitor's product catalog, puppeteer can be used to do this. To start, you must install puppeteer, to do so run the following command: "npm install puppeteer". This will download a bundled version of Chromium which takes up about 180 MB to 300 MB depending on your Operating System. If you wish to disable this and point puppeteer to an already downloaded version of chromium, you must set a few environment variables. This, however, is not recommended, if you truly wish to avoid downloading Chromium and puppeteer for this tutorial, you can rely on the puppeteer playground (https://try-puppeteer.appspot.com/).
  6. Nightmare - Nightmare is also a high-level browser automation library like Puppeteer, that uses Electron but is said to be roughly twice as faster as it's predecessor PhantomJS and more modern. If you dislike Puppeteer in some way or feel discouraged by the size of the Chromium bundle then Nightmare is an ideal choice. To start, installghtmare library by running the following command: npm install nightmare. Puppeteer and Nightmare are high-level browser automation libraries, that allow you to programmatically manipulate web applications as if a real person were interacting with it.
  
Document.querySelector()
  1. The Document method querySelector() returns the first Element within the document that matches the specified selector, or group of selectors. If no matches are found, null is returned.

Document.querySelectorAll()
  1. The Document method querySelectorAll() returns a static (not live) NodeList representing a list of the document's elements that match the specified group of selectors.
